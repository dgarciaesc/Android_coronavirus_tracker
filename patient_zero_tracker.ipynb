{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext,SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "import boto3\n",
    "from boto3.dynamodb.conditions import Key, Attr\n",
    "from pandas.io.json import json_normalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamo DB Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('dynamodb',endpoint_url='http://localhost:8000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dynamodb = boto3.resource('dynamodb', region_name='us-west-2', endpoint_url=\"http://localhost:8000\")\n",
    "\n",
    "for i in client.list_tables()[\"TableNames\"]:\n",
    "    client.delete_table(TableName=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_locations = dynamodb.create_table(\n",
    "    TableName='Locations',\n",
    "    KeySchema=[\n",
    "        {\n",
    "            'AttributeName': 'id',\n",
    "            'KeyType': 'HASH'  #Partition key\n",
    "        },\n",
    "        {\n",
    "            'AttributeName': 'description',\n",
    "            'KeyType': 'RANGE'  #Sort key\n",
    "        }\n",
    "    ],\n",
    "    AttributeDefinitions=[\n",
    "        {\n",
    "            'AttributeName': 'id',\n",
    "            'AttributeType': 'S'\n",
    "        },\n",
    "        {\n",
    "            'AttributeName': 'description',\n",
    "            'AttributeType': 'S'\n",
    "        },\n",
    "\n",
    "    ],\n",
    "    ProvisionedThroughput={\n",
    "        'ReadCapacityUnits': 10,\n",
    "        'WriteCapacityUnits': 10\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_locations_in_dynamo(locations_dict, table):\n",
    "    for k in locations_dict:\n",
    "        response = table.put_item(\n",
    "            Item={\n",
    "                'id': k,\n",
    "                 'description':locations_dict[k]\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Var Declaration\n",
    "#basepath is where the input files will be stored\n",
    "basepath=\"./test\"\n",
    "user_data={}\n",
    "locations_all={}\n",
    "locations_detail={}\n",
    "locations_name={}\n",
    "locations_times_pds={}\n",
    "locations_detail_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlyPlaces(every_stop):\n",
    "    if (every_stop.get(\"placeVisit\",\"null\") !=\"null\"):\n",
    "        return every_stop.get(\"placeVisit\",\"null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location_times_pd(locations):\n",
    "    locations_times=defaultdict()\n",
    "    for location,time in locations:\n",
    "        time[\"id_place\"]=location\n",
    "        locations_times[location+\"#\"+time[\"startTimestampMs\"]]=time\n",
    "    location_times_pd = pd.DataFrame.from_dict(locations_times,orient='index')\n",
    "    location_times_pd[\"startTimestampMs\"]=pd.to_numeric(location_times_pd[\"startTimestampMs\"])\n",
    "    location_times_pd[\"endTimestampMs\"]=pd.to_numeric(location_times_pd[\"endTimestampMs\"])\n",
    "    location_times_pd.index.name=\"id_starttime\"\n",
    "    return location_times_pd\n",
    "\n",
    "def get_location_times_dict(locations):\n",
    "    locations_times=defaultdict()\n",
    "    for location,time in locations:\n",
    "        time[\"id_place\"]=location\n",
    "        locations_times[location+\"#\"+time[\"startTimestampMs\"]]=time\n",
    "    return location_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Var Declaration\n",
    "basepath=\"./test\"\n",
    "user_data={}\n",
    "locations_all={}\n",
    "locations_detail={}\n",
    "locations_name={}\n",
    "locations_times_pds={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017_APRIL.json processed\n",
      "Guybrush_Threpwood_April copy 3.json processed\n",
      "2017_MARCH copy 2.json processed\n",
      "2017_DECEMBER copy 4.json processed\n",
      "Error processing 2017_JULY copy 7.json\n",
      "'timelineObjects'\n",
      "2017_JULY copy 7.json processed\n",
      "2017_MAY copy 3.json processed\n",
      "2017_AUGUST copy 7.json processed\n",
      "2017_MAY.json processed\n",
      "2017_JUNE copy 3.json processed\n",
      "Error processing 2017_JANUARY copy 2.json\n",
      "'timelineObjects'\n",
      "2017_JANUARY copy 2.json processed\n",
      "Sonya_Blade copy 7.json processed\n",
      "John_Doe_April copy 3.json processed\n",
      "2017_OCTOBER copy 3.json processed\n",
      "Error processing 2017_JULY copy.json\n",
      "'timelineObjects'\n",
      "2017_JULY copy.json processed\n",
      "2017_NOVEMBER copy 2.json processed\n",
      "David_Garcia_April copy 2.json processed\n",
      "David_Garcia_April copy 10.json processed\n",
      "John_Doe_April.json processed\n",
      "2017_NOVEMBER copy.json processed\n",
      "2017_FEBRUARY.json processed\n",
      "2017_OCTOBER.json processed\n",
      "David_Garcia_April copy 11.json processed\n",
      ".DS_Store processed\n",
      "David_Garcia_April copy 3.json processed\n",
      "2017_NOVEMBER copy 3.json processed\n",
      "John_Doe_April copy 2.json processed\n",
      "2017_OCTOBER copy 2.json processed\n",
      "Sonya_Blade copy 6.json processed\n",
      "Error processing 2017_JANUARY copy 3.json\n",
      "'timelineObjects'\n",
      "2017_JANUARY copy 3.json processed\n",
      "2017_MAY copy 2.json processed\n",
      "2017_AUGUST copy 6.json processed\n",
      "2017_JUNE copy 2.json processed\n",
      "David_Garcia_April copy.json processed\n",
      "2017_MARCH copy 3.json processed\n",
      "2017_DECEMBER copy 5.json processed\n",
      "2017_JUNE.json processed\n",
      "Sonya_Blade copy.json processed\n",
      "Error processing 2017_JULY copy 6.json\n",
      "'timelineObjects'\n",
      "2017_JULY copy 6.json processed\n",
      "Guybrush_Threpwood_April copy 2.json processed\n",
      "2017_OCTOBER copy 5.json processed\n",
      "John_Doe_April copy 5.json processed\n",
      "John_Doe_April copy.json processed\n",
      "2017_NOVEMBER copy 4.json processed\n",
      "2017_SEPTEMBER copy 6.json processed\n",
      "David_Garcia_April copy 4.json processed\n",
      "2017_APRIL copy 6.json processed\n",
      "2017_AUGUST.json processed\n",
      "2017_SEPTEMBER.json processed\n",
      "2017_MARCH copy.json processed\n",
      "Guybrush_Threpwood_April copy 5.json processed\n",
      "2017_DECEMBER copy 2.json processed\n",
      "2017_MARCH copy 4.json processed\n",
      "2017_MAY copy 5.json processed\n",
      "2017_JUNE copy 5.json processed\n",
      "David_Garcia_April copy 8.json processed\n",
      "Error processing 2017_JANUARY copy 4.json\n",
      "'timelineObjects'\n",
      "2017_JANUARY copy 4.json processed\n",
      "2017_DECEMBER copy.json processed\n",
      "2017_FEBRUARY copy 6.json processed\n",
      "2017_FEBRUARY copy 7.json processed\n",
      "Error processing 2017_JANUARY copy 5.json\n",
      "'timelineObjects'\n",
      "2017_JANUARY copy 5.json processed\n",
      "2017_MAY copy 4.json processed\n",
      "2017_JUNE copy 4.json processed\n",
      "David_Garcia_April copy 9.json processed\n",
      "Guybrush_Threpwood_April copy.json processed\n",
      "2017_DECEMBER copy 3.json processed\n",
      "2017_MARCH copy 5.json processed\n",
      "2017_AUGUST copy.json processed\n",
      "Guybrush_Threpwood_April copy 4.json processed\n",
      "2017_APRIL copy 7.json processed\n",
      "David_Garcia_April copy 5.json processed\n",
      "Guybrush_Threpwood_April.json processed\n",
      "2017_NOVEMBER copy 5.json processed\n",
      "2017_SEPTEMBER copy 7.json processed\n",
      "2017_OCTOBER copy 4.json processed\n",
      "John_Doe_April copy 4.json processed\n",
      "2017_APRIL copy 4.json processed\n",
      "David_Garcia_April copy 14.json processed\n",
      "Sonya_Blade.json processed\n",
      "David_Garcia_April copy 6.json processed\n",
      "2017_NOVEMBER copy 6.json processed\n",
      "2017_SEPTEMBER copy 4.json processed\n",
      "John_Doe_April copy 7.json processed\n",
      "2017_OCTOBER copy 7.json processed\n",
      "2017_APRIL copy.json processed\n",
      "2017_FEBRUARY copy 4.json processed\n",
      "Sonya_Blade copy 3.json processed\n",
      "Error processing 2017_JANUARY copy 6.json\n",
      "'timelineObjects'\n",
      "2017_JANUARY copy 6.json processed\n",
      "Error processing 2017_JANUARY copy.json\n",
      "'timelineObjects'\n",
      "2017_JANUARY copy.json processed\n",
      "2017_AUGUST copy 3.json processed\n",
      "2017_JUNE copy 7.json processed\n",
      "2017_MAY copy 7.json processed\n",
      "Error processing 2017_JULY copy 3.json\n",
      "'timelineObjects'\n",
      "2017_JULY copy 3.json processed\n",
      "2017_MARCH copy 6.json processed\n",
      "Error processing 2017_JULY.json\n",
      "'timelineObjects'\n",
      "2017_JULY.json processed\n",
      "Guybrush_Threpwood_April copy 7.json processed\n",
      "Guybrush_Threpwood_April copy 6.json processed\n",
      "Error processing 2017_JULY copy 2.json\n",
      "'timelineObjects'\n",
      "2017_JULY copy 2.json processed\n",
      "2017_MARCH copy 7.json processed\n",
      "2017_MAY copy.json processed\n",
      "2017_AUGUST copy 2.json processed\n",
      "2017_JUNE copy 6.json processed\n",
      "2017_MAY copy 6.json processed\n",
      "Error processing 2017_JANUARY copy 7.json\n",
      "'timelineObjects'\n",
      "2017_JANUARY copy 7.json processed\n",
      "2017_FEBRUARY copy 5.json processed\n",
      "Sonya_Blade copy 2.json processed\n",
      "2017_OCTOBER copy.json processed\n",
      "2017_SEPTEMBER copy.json processed\n",
      "John_Doe_April copy 6.json processed\n",
      "2017_OCTOBER copy 6.json processed\n",
      "2017_NOVEMBER copy 7.json processed\n",
      "2017_SEPTEMBER copy 5.json processed\n",
      "David_Garcia_April copy 7.json processed\n",
      "David_Garcia_April copy 15.json processed\n",
      "2017_APRIL copy 5.json processed\n",
      "Sonya_Blade copy 5.json processed\n",
      "2017_FEBRUARY copy 2.json processed\n",
      "2017_NOVEMBER.json processed\n",
      "2017_AUGUST copy 5.json processed\n",
      "Error processing 2017_JULY copy 5.json\n",
      "'timelineObjects'\n",
      "2017_JULY copy 5.json processed\n",
      "2017_DECEMBER copy 6.json processed\n",
      "2017_DECEMBER.json processed\n",
      "2017_APRIL copy 2.json processed\n",
      "David_Garcia_April copy 12.json processed\n",
      "2017_SEPTEMBER copy 2.json processed\n",
      "Error processing 2017_JANUARY.json\n",
      "'timelineObjects'\n",
      "2017_JANUARY.json processed\n",
      "2017_SEPTEMBER copy 3.json processed\n",
      "2017_JUNE copy.json processed\n",
      "2017_FEBRUARY copy.json processed\n",
      "David_Garcia_April copy 13.json processed\n",
      "2017_APRIL copy 3.json processed\n",
      "Error processing 2017_JULY copy 4.json\n",
      "'timelineObjects'\n",
      "2017_JULY copy 4.json processed\n",
      "David_Garcia_April.json processed\n",
      "2017_DECEMBER copy 7.json processed\n",
      "2017_AUGUST copy 4.json processed\n",
      "Sonya_Blade copy 4.json processed\n",
      "2017_MARCH.json processed\n",
      "2017_FEBRUARY copy 3.json processed\n",
      "CPU times: user 19.5 s, sys: 795 ms, total: 20.3 s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "i=0\n",
    "\n",
    "#Iterate through input jsons\n",
    "for entry in os.listdir(basepath):\n",
    "    if os.path.isfile(os.path.join(basepath, entry)):\n",
    "        with open(os.path.join(basepath, entry), 'r') as file:\n",
    "            #Filter non json files\n",
    "            if(os.path.splitext(entry)[-1].lower()==\".json\"):\n",
    "                user_data[entry] = json.load(file)\n",
    "                try:\n",
    "                    #Filter to get only main locations,discarding trips and infered locations\n",
    "                    locations_all[entry]= list(filter(onlyPlaces,user_data[entry][\"timelineObjects\"]))\n",
    "                    #For each location, we get the periods been there\n",
    "                    locations_detail_dict=[(i.get(\"placeVisit\",\"null\").get('location').get(\"placeId\"),i.get(\"placeVisit\",\"null\").get('duration')) for i in locations_all[entry]]\n",
    "                    locations_times_pds[entry]=get_location_times_pd(locations_detail_dict)\n",
    "                    \n",
    "                    #locations_detail[entry]= spark.createDataFrame(pd.DataFrame.from_dict(locations_detail_dict,orient='index'))\n",
    "                    #spark.createDataFrame(df1_pd.reset_index(drop=False))\n",
    "                    #We get the ids and store them in a local Dynamo DB (useful for GDPR)\n",
    "                    locations_name={i.get(\"placeVisit\",\"null\").get('location').get(\"placeId\"):i.get(\"placeVisit\",\"null\").get('location').get('name') for i in locations_all[entry]}\n",
    "                    insert_locations_in_dynamo(locations_name,table_locations)\n",
    "                except Exception as exc:\n",
    "                    print(\"Error processing \" +entry)\n",
    "                    print(exc)\n",
    "                    pass\n",
    "            print(entry + \" processed\")\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting combinations of all month file inputs\n",
    "combinations=(list(itertools.combinations(locations_all.keys(), 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Receives 2 dataframes (one per file entry) with locations_times format and returns a df with rows where both entries have been in the same time\n",
    "# at the same space\n",
    "def get_df_common_moments(df1_pd,entry_pd1,df2_pd,entry_pd2):\n",
    "    df_sol=pd.DataFrame(columns=[\"startTimestampMs_x\",\"endTimestampMs_x\",\"id_place\",\"startTimestampMs_y\",\"endTimestampMs_y\", \"people_involved\"])\n",
    "    df2=df1_pd.merge(df2_pd,on='id_place',how='left')\n",
    "    #df1 starts bef\n",
    "    df_cond_1 = df2[((df2[\"startTimestampMs_x\"] <= df2[\"startTimestampMs_y\"])&(df2[\"endTimestampMs_x\"] >= df2[\"startTimestampMs_y\"]))]\n",
    "    df_cond_2 = df2[(df2[\"startTimestampMs_x\"] <= df2[\"endTimestampMs_y\"])&(df2[\"startTimestampMs_x\"] >= df2[\"startTimestampMs_y\"])]\n",
    "    df_cond_1.append(df_cond_2)\n",
    "    df_cond_1= df_cond_1.assign(people_involved=entry_pd1+\" , \"+entry_pd2)\n",
    "    df_sol=df_cond_1\n",
    "    return df_sol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sol=pd.DataFrame(columns=[\"startTimestampMs_x\",\"endTimestampMs_x\",\"id_place\",\"startTimestampMs_y\",\"endTimestampMs_y\", \"people_involved\"])\n",
    "#Receives 2 dataframes (one per file entry) with locations_times format and returns a df with rows where both entries have been in the same time\n",
    "# at the same space\n",
    "get_df_common_moments(locations_times_pds[\"David_Garcia_April.json\"],\"David_Garcia_April.json\",locations_times_pds[\"Guybrush_Threpwood_April.json\"],'Guybrush_Threpwood_April.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_sol=pd.DataFrame(columns=[\"startTimestampMs_x\",\"endTimestampMs_x\",\"id_place\",\"startTimestampMs_y\",\"endTimestampMs_y\", \"people_involved\"])\n",
    "for combination in combinations:\n",
    "    print(combination)\n",
    "    df_sol_aux=get_df_common_moments(locations_times_pds[combination[0]],combination[0],locations_times_pds[combination[1]],combination[1])\n",
    "    df_sol=pd.concat([df_sol,df_sol_aux])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sol.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sol"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
